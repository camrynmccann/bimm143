---
title: "Class09_MiniProject"
author: 'Camryn McCann (PID: A15437387)'
date: "10/26/2021"
output: pdf_document
---

**To start off our mini-project, lets download and set up the data!**

```{r}
#First we can read the data file
read.csv("WisconsinCancer.csv")
```


Now that we have the data let's save it as fna.data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
#Use head function to check the table!
head(wisc.df)
```


We can omit the first column from our data frame because we will not be using it. 

```{r}
wisc.data <- wisc.df[,-1]
```

Last part of the data set up is to make a diagnosis vector for later use

```{r}
diagnosis <- as.factor(wisc.df[,1])
```

**Now, lets answer some data analysis questions**

> **Q1: How many observations are in this dataset?**

```{r}
dim(wisc.data)
```
There are 569 observations of 30 variables.

> **Q2. How many of the observations have a malignant diagnosis?**

```{r}
table(diagnosis)
```
212 observations have a malignant diagnosis.

> **Q3. How many variables/features in the data are suffixed with _mean?**

```{r}
colnames(wisc.data)
grep("_mean", colnames(wisc.data))
#the length vector allows us to find out how many 
length(grep("_mean", colnames(wisc.data)))
```
There are 10 variables in the data that are suffixed with _mean.


**Next lets perform some Principal Component Analysis (PCA)**

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Next we will do PCA

```{r}
#we must include a scale to account for variance
wisc.pr <- prcomp(wisc.data, scale=TRUE )
#lets check a summary 
summary(wisc.pr)
```


**Now lets answer some questions on our PCA**

> **Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?** 

0.4427 is the proportion of the original variance captured by PC1.

> **Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**

4 principal components are required to describe at least 70% of the original variance.

> **Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**

7 principal components are required to describe at least 90% of the original variance.


**Let's look at PCA visually !**

```{r}
biplot(wisc.pr)
```
**Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?** 
This plot is very messy and hard to read or analyze in any way because there is too much going on. 

Lets look at this on a score plot instead!

```{r}
plot(wisc.pr$x[,1:2], xlab= "PC1", ylab= "PC2", col= diagnosis)
```

**Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? ** 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], xlab= "PC1", ylab= "PC3", col= diagnosis)
```
Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner separation of the two groups.


Next, lets use a ggplot for better aesthetics!

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col= diagnosis) + 
  geom_point()
```

Furthermore, lets attempt to explain some of the variance. 

```{r}
# Calculate variance of each component
pr.var <-wisc.pr$sdev^2
head(pr.var)
```

Now let's calculate the variance explained by each prinicpal component. 

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve
```

We should look at this on a plot. 

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o", col = "Blue")
```
We can also look on a scree plot as well.

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE, col = "Blue")
axis(2, at=pve, labels=round(pve,2)*100 )
```
**Communicating the PCA Results Questions** 

> **Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


> **Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**

```{r}
summary(wisc.pr)
```

5 principal components are required to describe at least 80% of the original variance.


**Moving on to Hierarchical Clustering**

First lets scale our data. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Next, we need to calculate the Euclidean distances 

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method = "complete" )
```

> **Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

At a height of 19, there are 4 clusters. 


Let's continue to look more into clustering. 

```{r}
wisc.hclust.clusters <-cutree(wisc.hclust, k=4)
#lets look on a table
table(wisc.hclust.clusters, diagnosis)
```

>**Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?**

```{r}
table(cutree(wisc.hclust, k=2), diagnosis)
```

>**Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**

```{r}
plot(hclust(data.dist, method= "average"))
plot(hclust(data.dist, method= "single"))
plot(hclust(data.dist, method= "complete"))
plot(hclust(data.dist, method= "ward.D2"))
```

Looking at the data.dist set, out of the mehtods "average", "single", "complete", and "ward.D2", the "ward.D2" gives me my favorite resutls because the data is centered and clear. 

**Not lets combine methods and do clustering on our PCA results**

We can take our PCA results and cluster this is the space 'wisc.pr$x'

```{r}
wisc.pc.hclust<-hclust(dist(wisc.pr$x[,1:3]), method="ward.D2")
```

Now lets plot the dendrogram 
```{r}
plot(wisc.pc.hclust)
abline(h=70, col="red")
```


Now, lets cut the tree in k=2 groups 
```{r}
grps<-cutree(wisc.pc.hclust, k=2)
table(grps)
```


Cross table to compare diagnosis and cluster groups

```{r}
table(diagnosis,grps)
```

We can also visualize 'wisc.pr$x' on a plot.
*We used 1:3 in class to cover more data, although the workbook noted 1:2*

```{r}
plot(wisc.pr$x[,1:3], col=grps)
```


Instead lets color by diagnosis

```{r}
plot(wisc.pr$x[,1:3], col=diagnosis)
```

We can re-order the groups by making them a factor, so cluster 1 (malignant) is red, and cluster 2 (begnign) is black.

```{r}
g <- as.factor(grps)
g <- relevel(g,2)
levels(g)
```

Let's check the plot with our re-ordered factor.

```{r}
plot(wisc.pr$x[,1:3], col=g)
```

Lets use the data along the first 7 PCs for clustering instead this time !

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
# lets cut this model into 2 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

Using table(), compare the results from your new hierarchical clustering model with the actual diagnoses.

>**Q15. How well does the newly created model with four clusters separate out the two diagnoses?**

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
This model seperates out the two diagnoses well. 

**It is important to consider Sensitivity/Specificity**

Accuracy - What proportion did we get correct if we call cluster 1 M and 2 B?

> **Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?** 

Using the numbers from 
```{r}
table(diagnosis,grps)
```


```{r}
(333+179)/nrow(wisc.data)
```

Sensitivity - a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples. In other words: TP/(TP+FN)

```{r}
179/(179+33)
```

Specificity -  a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign. In other words: TN/(TN+FN)

```{r}
333/(24+333)
```


**We can also use PCA as a predictor** 

Let's use some new data and see if PCA can predict it!

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```


Next, we can plot it.

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> **Q18. Which of these new patients should we prioritize for follow up based on your results?** 

Based on our results, we should prioritize patient 2 because the yare more likely to be in the malignant grouping. 