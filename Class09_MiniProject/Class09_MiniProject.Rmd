---
title: "Class09_MiniProject"
author: 'Camryn McCann (PID: A15437387)'
date: "10/26/2021"
output: pdf_document
---

**To start off our mini-project, lets download and set up the data!**

```{r}
#First we can read the data file
read.csv("WisconsinCancer.csv")
```


Now that we have the data let's save it as fna.data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
#Use head function to check the table!
head(wisc.df)
```


We can omit the first column from our data frame because we will not be using it. 

```{r}
wisc.data <- wisc.df[,-1]
```

Last part of the data set up is to make a diagnosis vector for later use

```{r}
diagnosis <- as.factor(wisc.df[,1])
```

**Now, lets answer some data analysis questions**

> **Q1: How many observations are in this dataset?**

```{r}
dim(wisc.data)
```
There are 569 observations of 30 variables.

> **Q2. How many of the observations have a malignant diagnosis?**

```{r}
table(diagnosis)
```
212 observations have a malignant diagnosis.

> **Q3. How many variables/features in the data are suffixed with _mean?**

```{r}
colnames(wisc.data)
grep("_mean", colnames(wisc.data))
#the length vector allows us to find out how many 
length(grep("_mean", colnames(wisc.data)))
```
There are 10 variables in the data that are suffixed with _mean.


**Next lets perform some Principal Component Analysis (PCA)**

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Next we will do PCA

```{r}
#we must include a scale to account for variance
wisc.pr <- prcomp(wisc.data, scale=TRUE )
#lets check a summary 
summary(wisc.pr)
```


**Now lets answer some questions on our PCA**

> **Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?** 

0.4427 is the proportion of the original variance captured by PC1.

> **Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**

4 principal components are required to describe at least 70% of the original variance.

> **Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**

7 principal components are required to describe at least 90% of the original variance.


**Let's look at PCA visually !**

```{r}
biplot(wisc.pr)
```
**Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?** 
This plot is very messy and hard to read or analyze in any way because there is too much going on. 

Lets look at this on a score plot instead!

```{r}
plot(wisc.pr$x[,1:2], xlab= "PC1", ylab= "PC2", col= diagnosis)
```

**Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? ** 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], xlab= "PC1", ylab= "PC3", col= diagnosis)
```
Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner separation of the two groups.


Next, lets use a ggplot for better aesthetics!

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col= diagnosis) + 
  geom_point()
```

Furthermore, lets attempt to explain some of the variance. 

```{r}
# Calculate variance of each component
pr.var <-wisc.pr$sdev^2
head(pr.var)
```

Now let's calculate the variance explained by each prinicpal component. 

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
pve
```

We should look at this on a plot. 

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o", col = "Blue")
```
We can also look on a scree plot as well.

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE, col = "Blue")
axis(2, at=pve, labels=round(pve,2)*100 )
```
**Communicating the PCA Results Questions** 

> **Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


> **Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**

```{r}
summary(wisc.pr)
```

5 principal components are required to describe at least 80% of the original variance.


**Moving on to Hierarchical Clustering**

First lets scale our data. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Next, we need to calculate the Euclidean distances 

```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method = "complete" )
```

> **Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

At a height of 19, there are 4 clusters. 